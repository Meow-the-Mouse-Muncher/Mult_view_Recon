import os
import torch
import lightning as L
from torch import nn
from models.lfnr import LFNR
from dataset.LF_dataset import LFDataModule
from configs.config import get_config
from lightning.pytorch.loggers import TensorBoardLogger
from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
import h5py # ç¡®ä¿é¡¶éƒ¨å¯¼å…¥
torch.set_float32_matmul_precision('high')

class LFModule(L.LightningModule):
    """Lightning æ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äºè®­ç»ƒ LFNRã€‚"""
    def __init__(self, config, n_rays=4096):
        super().__init__()
        # ç›´æ¥ä¿å­˜æ•´ä¸ª config å¯¹è±¡
        self.save_hyperparameters(config)
        self.config = config
        self.n_rays = n_rays
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼šä»…ä¼ ä¸€ä¸ª config å¯¹è±¡
        self.model = LFNR(config=config)
        
        # æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        
        # æŒ‡æ ‡åˆå§‹åŒ–
        metrics_kwargs = {"data_range": 1.0}
        self.psnr = PeakSignalNoiseRatio(**metrics_kwargs)
        self.ssim = StructuralSimilarityIndexMeasure(**metrics_kwargs)
        
        # [æ–°å¢] éªŒè¯é›†ç»“æœç¼“å­˜
        # ç»“æ„: { file_idx: { 'preds': [], 'gts': [], 'center': ... } }
        self.val_outputs = {}

    def forward(self, batch):
        return self.model.forward(batch)

    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤ (Sparse Ray Training)"""
        # 1. å‰å‘ä¼ æ’­
        # è¿”å›å€¼: é¢„æµ‹RGB, é‡å åŒºRGB(å¦‚æœ‰)
        pred_rgb, rgb_overlap = self(batch)
        gt_rgb = batch['gt_rgb'] # [B, n_rays, 3]
        
        # 2. è®¡ç®—æŸè€—
        # ä¸»é¢„æµ‹æŸå¤±
        loss_pred = self.mse_loss(pred_rgb, gt_rgb)
        
        # é‡å åŒºåŸŸ/è¾…åŠ©æŸå¤± (å¦‚æœæ¨¡å‹æ”¯æŒ)
        loss_overlap = self.mse_loss(rgb_overlap, gt_rgb)
        # æ€»æŸå¤± (L2æƒé‡è¡°å‡å·²åœ¨AdamWä¸­å¤„ç†)
        loss = loss_pred + loss_overlap
        
        # 3. è®°å½•æ—¥å¿—
        self.log('train/loss', loss, prog_bar=True, sync_dist=True)
        self.log('train/loss_pred', loss_pred, sync_dist=True)
        self.log('train/loss_overlap', loss_overlap, sync_dist=True)
            
        return loss

    # === DDP å…¼å®¹çš„éªŒè¯é€»è¾‘ ===
    
    def on_validation_epoch_start(self):
        # æ”¹ç”¨åˆ—è¡¨å­˜å‚¨ï¼Œæ–¹ä¾¿ stack
        self.val_step_outputs = [] 

    def validation_step(self, batch, batch_idx):
        pred_chunk, _ = self(batch)
        # ä»…æ”¶é›†é¢„æµ‹ã€GTå’Œç´¢å¼•ï¼Œä¸æ”¶é›† Center å›¾
        self.val_step_outputs.append({
            'pred': pred_chunk.detach(),
            'gt': batch['gt_rgb'].detach(),
            'file_idx': batch['meta_file_idx'],
            'start_idx': batch['meta_start']
        })

    def on_validation_epoch_end(self):
        """
        å…¨å¡æ”¶é›† -> CPU æ‹¼å›¾ -> è®¡ç®—æŒ‡æ ‡ -> å¯è§†åŒ–
        """
        
        # [ä¿®å¤] å¦‚æœæ˜¯åœ¨è¿›è¡Œ Sanity Check (åªè·‘å‡ ä¸ª batch), æ•°æ®è‚¯å®šä¸å…¨
        # æ­¤æ—¶å¼ºåˆ¶è·³è¿‡ç¹é‡çš„æ‹¼å›¾å’Œå¯è§†åŒ–é€»è¾‘
        if self.trainer.sanity_checking:
            self.val_step_outputs.clear()
            return
            
        # 1. åœ¨å•å¡å†…éƒ¨ Stack èµ·æ¥
        if not self.val_step_outputs: return
        
        # 1. DDP æ”¶é›†
        local_preds = torch.cat([x['pred'] for x in self.val_step_outputs], dim=0)
        local_gts = torch.cat([x['gt'] for x in self.val_step_outputs], dim=0)
        local_fidxs = torch.cat([x['file_idx'] for x in self.val_step_outputs], dim=0)
        local_starts = torch.cat([x['start_idx'] for x in self.val_step_outputs], dim=0)
        
        global_preds = self.all_gather(local_preds).view(-1, local_preds.shape[1], 3).cpu()
        global_gts = self.all_gather(local_gts).view(-1, local_gts.shape[1], 3).cpu()
        global_fidxs = self.all_gather(local_fidxs).view(-1).cpu()
        global_starts = self.all_gather(local_starts).view(-1).cpu()

        # 2. ä»…åœ¨ Rank 0 æ‹¼å›¾ã€è®¡ç®—æŒ‡æ ‡å’Œå¯è§†åŒ–
        if self.global_rank == 0:
            val_h5_files = self.trainer.datamodule.val_dataset.h5_files
            unique_files = torch.unique(global_fidxs)
            
            # [æ–°å¢] ç´¯åŠ å™¨
            total_psnr = 0.0
            total_ssim = 0.0
            num_images = 0

            for fid in unique_files:
                if fid < 0: continue
                mask = (global_fidxs == fid)
                
                # æ‹¼å›¾é€»è¾‘
                sort_idx = torch.argsort(global_starts[mask])
                img_p = global_preds[mask][sort_idx].reshape(-1, 3)
                img_g = global_gts[mask][sort_idx].reshape(-1, 3)
                
                H = int(img_p.shape[0]**0.5)
                if H*H != img_p.shape[0]: continue
                
                # è½¬æ¢ä¸ºå›¾åƒç»´åº¦ [1, 3, H, W]
                view_p = img_p.view(1, H, H, 3).permute(0, 3, 1, 2).clamp(0, 1).to(self.device)
                view_g = img_g.view(1, H, H, 3).permute(0, 3, 1, 2).clamp(0, 1).to(self.device)
                
                # === [æ–°å¢] è®¡ç®—å•å¼ å›¾çš„æŒ‡æ ‡ ===
                cur_psnr = self.psnr(view_p, view_g)
                cur_ssim = self.ssim(view_p, view_g)
                
                total_psnr += cur_psnr
                total_ssim += cur_ssim
                num_images += 1
                
                # å¯è§†åŒ–ç¬¬ä¸€å¼ å›¾
                if num_images == 1:
                    h5_path = val_h5_files[fid]
                    with h5py.File(h5_path, 'r') as f:
                        center_img = torch.from_numpy(f['occ_center/rgb'][:]).float() / 255.0
                        center_view = center_img.permute(2, 0, 1).unsqueeze(0).to(self.device)
                    
                    if center_view.shape[2:] != view_p.shape[2:]:
                        print(f"[Warning] Skip Vis: Shape mismatch Ref{center_view.shape} vs Pred{view_p.shape}")
                    else:
                        grid = torch.cat([center_view, view_p, view_g], dim=3)
                        self.logger.experiment.add_image('val/Comparison', grid[0], self.global_step)
            
            # === [æ–°å¢] è®°å½•å¹³å‡æŒ‡æ ‡ ===
            if num_images > 0:
                avg_psnr = total_psnr / num_images
                avg_ssim = total_ssim / num_images
                # rank_zero_only=True é¿å…å¤šå¡é‡å¤è®°å½•
                self.log('val/psnr', avg_psnr, rank_zero_only=True)
                self.log('val/ssim', avg_ssim, rank_zero_only=True)

        # æ¸…ç©º
        self.val_step_outputs.clear()

    # === éªŒè¯é€»è¾‘ç»“æŸ ===
    
    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤ï¼ˆé€»è¾‘åŒ Valï¼‰"""
        return self.validation_step(batch, batch_idx)

    def configure_optimizers(self):
        """é…ç½®å¸¦çº¿æ€§é¢„çƒ­å’Œä½™å¼¦é€€ç«çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.config.train.lr_init,
            weight_decay=self.config.train.weight_decay,
            betas=(0.9, 0.98),
            eps=1e-9
        )
        
        warmup_steps = self.config.train.warmup_steps
        max_steps = self.config.train.max_steps
        
        # 1. çº¿æ€§é¢„çƒ­è°ƒåº¦å™¨: åœ¨ warmup_steps å†…ä» lr_init * 0.01 å¢åŠ åˆ° lr_init
        scheduler_warmup = torch.optim.lr_scheduler.LinearLR(
            optimizer, 
            start_factor=0.01, 
            total_iters=warmup_steps
        )
        
        # 2. ä½™å¼¦é€€ç«è°ƒåº¦å™¨: ä» warmup_steps å¼€å§‹ï¼Œåœ¨å‰©ä½™æ­¥æ•°å†…é™è‡³ lr_final
        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=max_steps - warmup_steps,
            eta_min=self.config.train.lr_final
        )
        
        # 3. é¡ºåºç»„åˆè°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.SequentialLR(
            optimizer,
            schedulers=[scheduler_warmup, scheduler_cosine],
            milestones=[warmup_steps]
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step", # å…³é”®ï¼šè®¾ç½®ä¸ºæŒ‰ step æ›´æ–°
            },
        }

    # def on_after_backward(self):
    #     # ä»…åœ¨è®­ç»ƒçš„ç¬¬ä¸€æ­¥è¿è¡Œä¸€æ¬¡æ£€æŸ¥
    #     if self.global_step == 0:
    #         print("\n" + "="*50)
    #         print("æ­£åœ¨æ£€æµ‹æœªä½¿ç”¨çš„æ¨¡å‹å‚æ•° (grad is None):")
    #         unused_params = []
    #         for name, param in self.named_parameters():
    #             if param.grad is None:
    #                 unused_params.append(name)
    #                 print(f"ğŸš© æœªä½¿ç”¨çš„å‚æ•°: {name}")
            
    #         if not unused_params:
    #             print("âœ… å®Œç¾ï¼æ‰€æœ‰å‚æ•°éƒ½å‚ä¸äº†æ¢¯åº¦è®¡ç®—ã€‚")
    #         else:
    #             print(f"\nå…±å‘ç° {len(unused_params)} ä¸ªæœªä½¿ç”¨çš„å‚æ•°ã€‚")
    #         print("="*50 + "\n")

if __name__ == "__main__":
    print("=== å¼€å§‹è®­ç»ƒ LFNR æ¨¡å‹ ===")
    
    # åŠ è½½é…ç½®
    config = get_config()
    mode = "rot_arc" # mode =[fix_line,rot_arc,rot_line]
    # 1. åˆå§‹åŒ–æ¨¡å‹åŒ…è£…å™¨
    model = LFModule(config=config, n_rays=config.train.num_rays)
    
    # 2. åˆ›å»ºæ•°æ®æ¨¡å—
    dm = LFDataModule(
        data_dir="data",
        model=mode,
        batch_size=1,
        num_workers=4,
        n_rays=config.train.num_rays,
        val_chunk_size=config.eval.chunk 
    )

    # åˆ›å»º Trainer
    trainer = L.Trainer(
        max_epochs=config.train.num_epochs,
        accelerator="gpu",
        devices=2,  
        strategy="ddp",
        logger=TensorBoardLogger("logs", name=mode, version=None), 
        callbacks=[
            L.pytorch.callbacks.ModelCheckpoint(
                dirpath=os.path.join("checkpoints", mode),
                filename="lfnr-{epoch:02d}",
                monitor="epoch",  # ç›‘æ§ epoch æ•°é‡
                mode="max",       # ä¿å­˜ epoch æœ€å¤§çš„ï¼ˆä¹Ÿå°±æ˜¯æœ€æ–°çš„ï¼‰
                save_top_k=4,
                every_n_epochs=5
            ),
            L.pytorch.callbacks.LearningRateMonitor(logging_interval="epoch")
        ],
        log_every_n_steps=20,
        check_val_every_n_epoch=1, 
    )


    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # --- å¢åŠ æ–­ç‚¹é‡è®­é€»è¾‘ ---
    ckpt_dir = os.path.join("checkpoints", mode)
    last_ckpt = None
    if os.path.exists(ckpt_dir):
        # å¯»æ‰¾ç›®å½•ä¸‹æ‰€æœ‰çš„ .ckpt æ–‡ä»¶
        ckpts = [os.path.join(ckpt_dir, f) for f in os.listdir(ckpt_dir) if f.endswith('.ckpt')]
        if ckpts:
            # æ‰¾åˆ°æœ€åä¿®æ”¹çš„æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯æœ€è¿‘ä¿å­˜çš„ï¼‰
            last_ckpt = max(ckpts, key=os.path.getmtime)
            print(f"æ£€æµ‹åˆ°æ–­ç‚¹æ–‡ä»¶ï¼Œå°†ä»æ­¤å¤„æ¢å¤è®­ç»ƒ: {last_ckpt}")

    # å¼€å§‹è®­ç»ƒ (ä¼ å…¥ ckpt_path å‚æ•°)
    trainer.fit(model, dm, ckpt_path=last_ckpt)
    
    # # å¯é€‰ï¼šæµ‹è¯•æœ€ä½³æ¨¡å‹
    # trainer.test(model, dm, ckpt_path="best")
    
    print("=== è®­ç»ƒå®Œæˆ ===")