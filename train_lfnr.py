import os
import torch
import lightning as L
from torch import nn
from models.lfnr import LFNR
from dataset.LF_dataset import LFDataModule
from configs.config import get_config
from lightning.pytorch.loggers import TensorBoardLogger
from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure

class LFModule(L.LightningModule):
    """Lightning æ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äºè®­ç»ƒ LFNRã€‚"""
    def __init__(self, config, n_rays=4096):
        super().__init__()
        # ç›´æ¥ä¿å­˜æ•´ä¸ª config å¯¹è±¡
        self.save_hyperparameters(config)
        self.config = config
        self.n_rays = n_rays
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼šä»…ä¼ ä¸€ä¸ª config å¯¹è±¡
        self.model = LFNR(config=config)
        
        # æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        
        # æŒ‡æ ‡
        self.psnr = PeakSignalNoiseRatio(data_range=1.0)
        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0)

    def forward(self, batch):
        return self.model.forward(batch)

    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤ (Sparse Ray Training)"""
        # 1. å‰å‘ä¼ æ’­
        # è¿”å›å€¼: é¢„æµ‹RGB, é‡å åŒºRGB(å¦‚æœ‰)
        pred_rgb, rgb_overlap = self(batch)
        gt_rgb = batch['gt_rgb'] # [B, n_rays, 3]
        
        # 2. è®¡ç®—æŸè€—
        # ä¸»é¢„æµ‹æŸå¤±
        loss_pred = self.mse_loss(pred_rgb, gt_rgb)
        
        # é‡å åŒºåŸŸ/è¾…åŠ©æŸå¤± (å¦‚æœæ¨¡å‹æ”¯æŒ)
        loss_overlap = self.mse_loss(rgb_overlap, gt_rgb)
        # æ€»æŸå¤± (L2æƒé‡è¡°å‡å·²åœ¨AdamWä¸­å¤„ç†)
        loss = loss_pred + loss_overlap
        
        # 3. è®°å½•æ—¥å¿—
        self.log('train/loss', loss, prog_bar=True, sync_dist=True)
        self.log('train/loss_pred', loss_pred, sync_dist=True)
        self.log('train/loss_overlap', loss_overlap, sync_dist=True)
            
        return loss

    def validation_step(self, batch, batch_idx):
        pass
        # """éªŒè¯æ­¥éª¤ (é€šå¸¸æ¸²æŸ“å…¨å›¾)"""
        # # Batch Size åœ¨éªŒè¯æ—¶åº”è¯¥ä¸º 1ï¼Œå› ä¸ºå…¨å›¾å…‰çº¿ H*W å¾ˆå¤§
        # # batch['gt_rgb']: [1, H*W, 3]
        
        # # 1. å‰å‘ä¼ æ’­ (Chunking å¤„ç†ä»¥é˜² OOM)
        # # æ³¨æ„ï¼šç”±äº sampling_grid å·²ç»æ˜¯ [1, N, H*W, 2]ï¼Œæˆ‘ä»¬éœ€è¦å¯¹ rays å’Œ grid è¿›è¡Œåˆ‡ç‰‡
        # chunk_size = 2048 # å¼ºè¡Œæ”¹å°è¯•è¯•ï¼ŒåŸå…ˆå¯èƒ½æ˜¯ 8192 å¤ªå¤§äº†
        # view_img = batch['occ_center_rgb'] 
        # B, total_pixels, _ = batch['gt_rgb'].shape
        # assert B == 1, "éªŒè¯/æµ‹è¯•æ—¶ Batch Size å¿…é¡»ä¸º 1"
        
        # all_pred_rgb = []
        
        # # é€å—è¿›è¡Œæ¨ç†
        # for i in range(0, total_pixels, chunk_size):
        #     end = min(i + chunk_size, total_pixels)
            
        #     # æ„å»ºä¸€ä¸ª mini-batch å­—å…¸
        #     chunk_batch = {
        #         # [1, chunk, 3]
        #         'gt_rays_o': batch['gt_rays_o'][:, i:end, :],
        #         'gt_rays_d': batch['gt_rays_d'][:, i:end, :],
        #         'pts_3d':    batch['pts_3d'][:, i:end, :],
                
        #         # å‚è€ƒä¿¡æ¯éƒ¨åˆ†
        #         'occ_rgb': batch['occ_rgb'], 

        #         # grid and rays need slicing
        #         'sampling_grid': batch['sampling_grid'][:, :, i:end, :],
        #         'occ_rays_d':    batch['occ_rays_d'][:, :, i:end, :],
        #         'occ_rays_o':    batch['occ_rays_o'][:, :, i:end, :]
        #     }
            
        #     # é¢„æµ‹
        #     with torch.no_grad():
        #         pred_chunk = self(chunk_batch)[0] 
        #         # å…³é”®ä¿®æ”¹ï¼šç«‹å³ç”± GPU è½¬å­˜åˆ° CPUï¼Œè…¾å‡ºæ˜¾å­˜ç»™ä¸‹ä¸€å—
        #         all_pred_rgb.append(pred_chunk.cpu()) 
                
        # # æ‹¼æ¥ç»“æœ (åœ¨ CPU ä¸Šè¿›è¡Œ)
        # pred_rgb = torch.cat(all_pred_rgb, dim=1).to(self.device) # å¦‚æœéœ€è¦è®¡ç®— loss å†è½¬å›å»ï¼Œæˆ–è€…ç›´æ¥åœ¨ CPU ç®— PSNR
        
        # # ä¼˜åŒ–ï¼šä¸ºäº†è®¡ç®— PSNRï¼ŒæŠŠ gt_rgb ä¹Ÿè½¬åˆ° CPU ç®—ï¼Œå½»åº•çœæ˜¾å­˜
        # gt_rgb_cpu = batch['gt_rgb'].cpu()
        # pred_rgb_cpu = torch.cat(all_pred_rgb, dim=1) # å·²ç»åœ¨ CPU ä¸Šäº†
        
        # # 2. è®¡ç®— PSNR (æ¨èä½¿ç”¨ torchmetrics çš„å‡½æ•°å¼æ¥å£ï¼Œæˆ–è€…ä¸´æ—¶æ–°å»ºå¯¹è±¡ï¼Œé¿å…è®¾å¤‡å†²çª)
        # # æ–¹å¼ A: ç›´æ¥æ‰‹åŠ¨è®¡ç®— MSE è½¬ PSNR (æœ€å¿«ï¼Œæ— ä¾èµ–)
        # mse = torch.mean((pred_rgb_cpu.clamp(0, 1) - gt_rgb_cpu.clamp(0, 1)) ** 2)
        # psnr_val = -10.0 * torch.log10(mse)
        
        # self.log('val/psnr', psnr_val, on_epoch=True, prog_bar=True, sync_dist=True)

        # # 3. è®°å½•ç¬¬ä¸€å¼ å›¾åƒ (ä»…åœ¨ batch_idx == 0 æ—¶æ‰§è¡Œ)
        # if batch_idx == 0:
        #     H, W = batch['H'].item(), batch['W'].item()
            
        #     # å‡†å¤‡å›¾åƒæ•°æ®: [1, 3, H, W]
        #     # ä¸€å¼ å›¾æ˜¾å­˜å ç”¨å¾ˆå°ï¼Œä¸ä¼š OOM
        #     p_img_gpu = pred_rgb_cpu[0].view(H, W, 3).permute(2, 0, 1).clamp(0, 1).unsqueeze(0).to(self.device)
        #     g_img_gpu = gt_rgb_cpu[0].view(H, W, 3).permute(2, 0, 1).clamp(0, 1).unsqueeze(0).to(self.device)
        #     c_img_gpu = batch['occ_center_rgb'].to(self.device)
        #     # ç¡®ä¿ç»´åº¦ä¸€è‡´ (æœ‰æ—¶ center å›¾å¯èƒ½æ˜¯ [B, H, W, 3] æˆ–è€…æ²¡æœ‰ batch ç»´)
        #     if c_img_gpu.ndim == 3: c_img_gpu = c_img_gpu.unsqueeze(0)
        #     if c_img_gpu.shape[-1] == 3: c_img_gpu = c_img_gpu.permute(0, 3, 1, 2) # [1, 3, H, W]
            
        #     # ä½¿ç”¨ self.ssim (åœ¨ GPU) è®¡ç®—
        #     ssim_val = self.ssim(p_img_gpu, g_img_gpu)
        #     self.log('val/ssim', ssim_val, on_epoch=True)
        #     concat_img = torch.cat([c_img_gpu, p_img_gpu, g_img_gpu], dim=3)

        #     # TensorBoard è®°å½• (ä¸éœ€è¦ GPUï¼Œå–å› CPU)
        #     self.logger.experiment.add_image('val/View_Pred_GT', concat_img[0].cpu(), self.global_step)
            
        # return psnr_val

    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤ï¼ˆé€»è¾‘åŒ Valï¼‰"""
        return self.validation_step(batch, batch_idx)

    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.config.train.lr_init,
            weight_decay=self.config.train.weight_decay,
            betas=(0.9, 0.999)
        )
        
        # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=self.config.train.num_epochs,
            eta_min=self.config.train.lr_final
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "epoch",
                "frequency": 1,
            },
        }

    # def on_after_backward(self):
    #     # ä»…åœ¨è®­ç»ƒçš„ç¬¬ä¸€æ­¥è¿è¡Œä¸€æ¬¡æ£€æŸ¥
    #     if self.global_step == 0:
    #         print("\n" + "="*50)
    #         print("æ­£åœ¨æ£€æµ‹æœªä½¿ç”¨çš„æ¨¡å‹å‚æ•° (grad is None):")
    #         unused_params = []
    #         for name, param in self.named_parameters():
    #             if param.grad is None:
    #                 unused_params.append(name)
    #                 print(f"ğŸš© æœªä½¿ç”¨çš„å‚æ•°: {name}")
            
    #         if not unused_params:
    #             print("âœ… å®Œç¾ï¼æ‰€æœ‰å‚æ•°éƒ½å‚ä¸äº†æ¢¯åº¦è®¡ç®—ã€‚")
    #         else:
    #             print(f"\nå…±å‘ç° {len(unused_params)} ä¸ªæœªä½¿ç”¨çš„å‚æ•°ã€‚")
    #         print("="*50 + "\n")

if __name__ == "__main__":
    print("=== å¼€å§‹è®­ç»ƒ LFNR æ¨¡å‹ ===")
    
    # åŠ è½½é…ç½®
    config = get_config()
    mode = "rot_arc" # mode =[fix_line,rot_arc,rot_line]
    # 1. åˆå§‹åŒ–æ¨¡å‹åŒ…è£…å™¨
    model = LFModule(config=config, n_rays=config.train.num_rays)
    
    # 2. åˆ›å»ºæ•°æ®æ¨¡å—
    dm = LFDataModule(
        data_dir="data",
        model=mode,
        batch_size=1, # å•ä¸ªgpuä¸Šçš„batch size
        num_workers=4,
        n_rays=config.train.num_rays
    )

    # åˆ›å»º Trainer
    trainer = L.Trainer(
        max_epochs=config.train.num_epochs,
        accelerator="gpu",
        devices=2,  
        strategy="ddp",
        logger=TensorBoardLogger("logs", name=mode, version=None), 
        callbacks=[
            L.pytorch.callbacks.ModelCheckpoint(
                dirpath=os.path.join("checkpoints", mode),
                filename="lfnr-{epoch:02d}",
                monitor="epoch",  # ç›‘æ§ epoch æ•°é‡
                mode="max",       # ä¿å­˜ epoch æœ€å¤§çš„ï¼ˆä¹Ÿå°±æ˜¯æœ€æ–°çš„ï¼‰
                save_top_k=4,
                every_n_epochs=5
            ),
            L.pytorch.callbacks.LearningRateMonitor(logging_interval="epoch")
        ],
        log_every_n_steps=20,
        check_val_every_n_epoch=5, 
    )


    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # --- å¢åŠ æ–­ç‚¹é‡è®­é€»è¾‘ ---
    ckpt_dir = os.path.join("checkpoints", mode)
    last_ckpt = None
    if os.path.exists(ckpt_dir):
        # å¯»æ‰¾ç›®å½•ä¸‹æ‰€æœ‰çš„ .ckpt æ–‡ä»¶
        ckpts = [os.path.join(ckpt_dir, f) for f in os.listdir(ckpt_dir) if f.endswith('.ckpt')]
        if ckpts:
            # æ‰¾åˆ°æœ€åä¿®æ”¹çš„æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯æœ€è¿‘ä¿å­˜çš„ï¼‰
            last_ckpt = max(ckpts, key=os.path.getmtime)
            print(f"æ£€æµ‹åˆ°æ–­ç‚¹æ–‡ä»¶ï¼Œå°†ä»æ­¤å¤„æ¢å¤è®­ç»ƒ: {last_ckpt}")

    # å¼€å§‹è®­ç»ƒ (ä¼ å…¥ ckpt_path å‚æ•°)
    trainer.fit(model, dm, ckpt_path=last_ckpt)
    
    # å¯é€‰ï¼šæµ‹è¯•æœ€ä½³æ¨¡å‹
    trainer.test(model, dm, ckpt_path="best")
    
    print("=== è®­ç»ƒå®Œæˆ ===")