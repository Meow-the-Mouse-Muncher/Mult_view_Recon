import os
import time
import torch
import lightning as L
from torch import nn
from models.lfnr_DA import LFNR
from dataset.LF_DA_dataset import LFDataModule
from configs.config_DA import get_config
from lightning.pytorch.loggers import TensorBoardLogger
from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
import h5py 
from torchvision.utils import save_image

torch.set_float32_matmul_precision('high')

class LFModule(L.LightningModule):
    """Lightning æ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äºè®­ç»ƒ LFNRï¼ˆåŠ¨æ€æœ€è¿‘Kç›¸æœºç‰ˆæœ¬ï¼‰ã€‚"""
    def __init__(self, config, n_rays=4096, save_dir="pred_data"):
        super().__init__()
        self.save_hyperparameters(config)
        self.config = config
        self.n_rays = n_rays
        self.save_dir = save_dir
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = LFNR(config=config)
        
        # æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        
        # æŒ‡æ ‡åˆå§‹åŒ–
        metrics_kwargs = {"data_range": 1.0}
        self.psnr = PeakSignalNoiseRatio(**metrics_kwargs)
        self.ssim = StructuralSimilarityIndexMeasure(**metrics_kwargs)
        
        # ç¼“å­˜
        self.val_step_outputs = []
        self.test_step_outputs = []

    def forward(self, batch):
        return self.model.forward(batch)

    def training_step(self, batch, batch_idx):
        pred_rgb, rgb_overlap = self(batch)
        gt_rgb = batch['gt_rgb'] 
        
        loss_pred = self.mse_loss(pred_rgb, gt_rgb)
        loss_overlap = self.mse_loss(rgb_overlap, gt_rgb)
        loss = loss_pred + loss_overlap
        
        self.log('train/loss', loss, prog_bar=True, sync_dist=True)
        self.log('train/loss_pred', loss_pred, sync_dist=True)
        self.log('train/loss_overlap', loss_overlap, sync_dist=True)
            
        return loss

    # === DDP å…¼å®¹çš„éªŒè¯é€»è¾‘ ===
    
    def on_validation_epoch_start(self):
        self.val_step_outputs = [] 

    def validation_step(self, batch, batch_idx):
        pred_chunk, _ = self(batch)
        self.val_step_outputs.append({
            'pred': pred_chunk.detach(),
            'gt': batch['gt_rgb'].detach(),
            'file_idx': batch['meta_file_idx'],
            'start_idx': batch['meta_start']
        })

    def on_validation_epoch_end(self):
        """
        å…¨å¡æ”¶é›† -> CPU æ‹¼å›¾ -> è®¡ç®—æŒ‡æ ‡ -> å¯è§†åŒ– (Center | Pred | GT)
        """
        # å¦‚æœæ˜¯åœ¨è¿›è¡Œ Sanity Check (åªè·‘å‡ ä¸ª batch), æ•°æ®è‚¯å®šä¸å…¨
        # æ­¤æ—¶å¼ºåˆ¶è·³è¿‡ç¹é‡çš„æ‹¼å›¾å’Œå¯è§†åŒ–é€»è¾‘
        if self.trainer.sanity_checking:
            self.val_step_outputs.clear()
            return

        if not self.val_step_outputs:
            return
            
        # 1. DDP æ”¶é›†
        local_preds = torch.cat([x['pred'] for x in self.val_step_outputs], dim=0)
        local_gts = torch.cat([x['gt'] for x in self.val_step_outputs], dim=0)
        local_fidxs = torch.cat([x['file_idx'] for x in self.val_step_outputs], dim=0)
        local_starts = torch.cat([x['start_idx'] for x in self.val_step_outputs], dim=0)
        
        global_preds = self.all_gather(local_preds).view(-1, local_preds.shape[1], 3).cpu()
        global_gts = self.all_gather(local_gts).view(-1, local_gts.shape[1], 3).cpu()
        global_fidxs = self.all_gather(local_fidxs).view(-1).cpu()
        global_starts = self.all_gather(local_starts).view(-1).cpu()

        # 2. ä»…åœ¨ Rank 0 æ‹¼å›¾ã€è®¡ç®—æŒ‡æ ‡å’Œå¯è§†åŒ–
        if self.global_rank == 0:
            # éªŒè¯é˜¶æ®µ val_dataset è‚¯å®šå­˜åœ¨
            val_h5_files = self.trainer.datamodule.val_dataset.h5_files
            unique_files = torch.unique(global_fidxs)
            
            # [æ¢å¤] ç´¯åŠ å™¨
            total_psnr = 0.0
            total_ssim = 0.0
            num_images = 0

            for fid in unique_files:
                if fid < 0: continue
                mask = (global_fidxs == fid)
                
                # æ‹¼å›¾é€»è¾‘
                sort_idx = torch.argsort(global_starts[mask])
                img_p = global_preds[mask][sort_idx].reshape(-1, 3)
                img_g = global_gts[mask][sort_idx].reshape(-1, 3)
                
                H = int(img_p.shape[0]**0.5)
                if H*H != img_p.shape[0]: continue
                
                # è½¬æ¢ä¸ºå›¾åƒç»´åº¦ [1, 3, H, W]ï¼Œç§»å› GPU è®¡ç®—æŒ‡æ ‡
                view_p = img_p.view(1, H, H, 3).permute(0, 3, 1, 2).clamp(0, 1).to(self.device)
                view_g = img_g.view(1, H, H, 3).permute(0, 3, 1, 2).clamp(0, 1).to(self.device)
                
                # === [æ¢å¤] è®¡ç®—å•å¼ å›¾çš„æŒ‡æ ‡ ===
                cur_psnr = self.psnr(view_p, view_g)
                cur_ssim = self.ssim(view_p, view_g)
                
                total_psnr += cur_psnr
                total_ssim += cur_ssim
                num_images += 1
                
                # [æ¢å¤] å¯è§†åŒ–ç¬¬ä¸€å¼ å›¾ (Center | Pred | GT)
                if num_images == 1:
                    h5_path = val_h5_files[fid]
                    # å°è¯•è¯»å– Center View
                    center_view = None
                    try:
                        with h5py.File(h5_path, 'r') as f:
                            if 'occ_center/rgb' in f:
                                center_img = torch.from_numpy(f['occ_center/rgb'][:]).float() / 255.0
                                center_view = center_img.permute(2, 0, 1).unsqueeze(0).to(self.device)
                    except Exception as e:
                        print(f"[Vis Warning] Could not read center view: {e}")

                    # æ£€æŸ¥å½¢çŠ¶åŒ¹é…å¹¶æ‹¼å›¾
                    if center_view is not None and center_view.shape[2:] == view_p.shape[2:]:
                         # ä¸‰è”å›¾
                         grid = torch.cat([center_view, view_p, view_g], dim=3)
                    else:
                         # åªæœ‰ Pred å’Œ GT
                         grid = torch.cat([view_p, view_g], dim=3)

                    self.logger.experiment.add_image('val/Comparison', grid[0], self.global_step)
            
            # === [æ¢å¤] è®°å½•å¹³å‡æŒ‡æ ‡ ===
            if num_images > 0:
                avg_psnr = total_psnr / num_images
                avg_ssim = total_ssim / num_images
                # rank_zero_only=True é¿å…å¤šå¡é‡å¤è®°å½•
                self.log('val/psnr', avg_psnr, rank_zero_only=True)
                self.log('val/ssim', avg_ssim, rank_zero_only=True)

        # æ¸…ç©º
        self.val_step_outputs.clear()

    def on_test_epoch_start(self):
        self.test_step_outputs = []

    def test_step(self, batch, batch_idx):
        # å¿…é¡»ç‹¬ç«‹å®ç°ï¼Œå› ä¸ºæˆ‘ä»¬è¦ä¿å­˜ç”¨äºæ‹¼å›¾çš„æ‰€æœ‰ chunks
        pred_chunk, _ = self(batch)
        self.test_step_outputs.append({
            'pred': pred_chunk.detach(),
            'gt': batch['gt_rgb'].detach(),
            'file_idx': batch['meta_file_idx'], # å“ªå¼ å›¾
            'start_idx': batch['meta_start']    # å“ªä¸ªä½ç½®
        })

    def on_test_epoch_end(self):
        if not self.test_step_outputs: return

        # 1. èšåˆæ‰€æœ‰å¡ä¸Šçš„ Chunks
        local_preds = torch.cat([x['pred'] for x in self.test_step_outputs], dim=0)
        local_gts = torch.cat([x['gt'] for x in self.test_step_outputs], dim=0)
        local_fidxs = torch.cat([x['file_idx'] for x in self.test_step_outputs], dim=0)
        local_starts = torch.cat([x['start_idx'] for x in self.test_step_outputs], dim=0)

        # ç§»åŠ¨åˆ° CPU å¹¶èšåˆ (é˜²æ­¢ OOM)
        global_preds = self.all_gather(local_preds).view(-1, local_preds.shape[1], 3).cpu()
        global_gts = self.all_gather(local_gts).view(-1, local_gts.shape[1], 3).cpu()
        global_fidxs = self.all_gather(local_fidxs).view(-1).cpu()
        global_starts = self.all_gather(local_starts).view(-1).cpu()

        # 2. ä»…åœ¨ Rank 0 å¤„ç†æ‹¼å›¾å’Œä¿å­˜å›¾ç‰‡
        if self.global_rank == 0:
            # [ä¿®æ­£ç‚¹]ï¼šå®‰å…¨è·å– test filenames
            test_h5_files = []
            if hasattr(self.trainer.datamodule, 'test_dataset'):
                test_h5_files = self.trainer.datamodule.test_dataset.h5_files
            else:
                print("Warning: test_dataset not found in DataModule. Filenames will be unavailable.")

            unique_files = torch.unique(global_fidxs) # ä½¿ç”¨å…¨å±€ç´¢å¼•
            print(f"æ­£åœ¨å¤„ç† {len(unique_files)} å¼ æµ‹è¯•å›¾åƒ...")
            
            # ä½¿ç”¨ log æ–‡ä»¶è®°å½•æŒ‡æ ‡
            os.makedirs(self.save_dir, exist_ok=True)
            log_path = os.path.join(self.save_dir, "metrics.txt")
            
            with open(log_path, "w") as f:
                f.write("Filename, PSNR, SSIM\n")
                
                total_psnr = 0
                total_ssim = 0
                count = 0

                for fid in unique_files:
                    fid = int(fid.item())
                    if fid < 0: continue
                    
                    mask = (global_fidxs == fid)
                    
                    current_starts = global_starts[mask]
                    sort_idx = torch.argsort(current_starts)
                    
                    img_p = global_preds[mask][sort_idx].reshape(-1, 3)
                    img_g = global_gts[mask][sort_idx].reshape(-1, 3)
                    
                    H = int(img_p.shape[0]**0.5)
                    if H*H != img_p.shape[0]: 
                        print(f"Skipping fid={fid}, shape mismatch: {img_p.shape}")
                        continue
                    
                    # è½¬æ¢ä¸ºå›¾åƒç»´åº¦ (æ­¤æ—¶åœ¨ CPU ä¸Š)
                    view_p = img_p.view(1, H, H, 3).permute(0, 3, 1, 2).clamp(0, 1)
                    view_g = img_g.view(1, H, H, 3).permute(0, 3, 1, 2).clamp(0, 1)
                    
                    cur_psnr = self.psnr(view_p, view_g).item()
                    cur_ssim = self.ssim(view_p, view_g).item()
                    
                    total_psnr += cur_psnr
                    total_ssim += cur_ssim
                    count += 1
                    
                    # è·å–æ–‡ä»¶å
                    if fid < len(test_h5_files):
                        h5_path = test_h5_files[fid]
                        fname = os.path.splitext(os.path.basename(h5_path))[0]
                        
                        # [æ–°å¢] å°è¯•è¯»å– Center View ç”¨äºåŒæ­¥ä¸‰æ‹¼å›¾å¯è§†åŒ–
                        center_view = None
                        try:
                            with h5py.File(h5_path, 'r') as h5f:
                                if 'occ_center/rgb' in h5f:
                                    center_img = torch.from_numpy(h5f['occ_center/rgb'][:]).float() / 255.0
                                    center_view = center_img.permute(2, 0, 1).unsqueeze(0)
                        except Exception as e:
                            print(f"[Test Vis Warning] Could not read center view for {fname}: {e}")
                    else:
                        fname = f"unknown_{fid}"
                        center_view = None

                    # ä¿å­˜å›¾ç‰‡ (æ ¹æ®æ˜¯å¦æœ‰ center_view å†³å®šæ˜¯ä¸‰æ‹¼è¿˜æ˜¯åŒæ‹¼)
                    save_path = os.path.join(self.save_dir, f"{fname}.png")
                    if center_view is not None and center_view.shape[2:] == view_p.shape[2:]:
                        grid = torch.cat([center_view, view_p, view_g], dim=3)
                    else:
                        grid = torch.cat([view_p, view_g], dim=3)
                    
                    save_image(grid, save_path)
                    
                    log_str = f"{fname}, {cur_psnr:.4f}, {cur_ssim:.4f}"
                    f.write(log_str + "\n")
                
                if count > 0:
                    avg_psnr = total_psnr / count
                    avg_ssim = total_ssim / count
                    final_log = f"\nAverage: PSNR={avg_psnr:.4f}, SSIM={avg_ssim:.4f}\n"
                    print(final_log)
                    f.write(final_log)

        self.test_step_outputs.clear()

    def configure_optimizers(self):
        """é…ç½®å¸¦çº¿æ€§é¢„çƒ­å’Œä½™å¼¦é€€ç«çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.config.train.lr_init,
            weight_decay=self.config.train.weight_decay,
            betas=(0.9, 0.98),
            eps=1e-9
        )
        
        warmup_steps = self.config.train.warmup_steps
        max_steps = self.config.train.max_steps
        
        # 1. çº¿æ€§é¢„çƒ­è°ƒåº¦å™¨: åœ¨ warmup_steps å†…ä» lr_init * 0.01 å¢åŠ åˆ° lr_init
        scheduler_warmup = torch.optim.lr_scheduler.LinearLR(
            optimizer, 
            start_factor=0.01, 
            total_iters=warmup_steps
        )
        
        # 2. ä½™å¼¦é€€ç«è°ƒåº¦å™¨: ä» warmup_steps å¼€å§‹ï¼Œåœ¨å‰©ä½™æ­¥æ•°å†…é™è‡³ lr_final
        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=max_steps - warmup_steps,
            eta_min=self.config.train.lr_final
        )
        
        # 3. é¡ºåºç»„åˆè°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.SequentialLR(
            optimizer,
            schedulers=[scheduler_warmup, scheduler_cosine],
            milestones=[warmup_steps]
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "step",
            },
        }

if __name__ == "__main__":
    import argparse
    start_time = time.time()
    parser = argparse.ArgumentParser()
    parser.add_argument("--exp_name", type=str, default="DA", help="æ¶ˆèå®éªŒåç§°")
    parser.add_argument("--mode", type=str, default="rot_arc", help="æ•°æ®æ¨¡å¼: fix_line, rot_arc, rot_line, mix")
    args, _ = parser.parse_known_args()

    print(f"=== å¼€å§‹è®­ç»ƒ LFNR-DA æ¨¡å‹ï¼ˆåŠ¨æ€æœ€è¿‘Kç›¸æœºï¼‰| å®éªŒ: {args.exp_name} | æ¨¡å¼: {args.mode} ===")
    
    # 1. åŠ è½½å‚æ•°
    config = get_config()
    exp_name = args.exp_name
    mode = args.mode
    
    # 2. æ„é€ è·¯å¾„: å®éªŒå/mode
    result_save_dir = os.path.join("pred_data", exp_name, mode)
    checkpoint_dir = os.path.join("checkpoints", exp_name, mode)
    os.makedirs(result_save_dir, exist_ok=True)

    # åˆå§‹åŒ–æ¨¡å‹åŒ…è£…å™¨
    model = LFModule(
        config=config, 
        n_rays=config.train.num_rays,
        save_dir=result_save_dir
    )
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    dm = LFDataModule(
        data_dir="data",
        train_data_dir="data/train_data",
        test_data_dir="data/test_data",
        model=mode,
        batch_size=1,
        num_workers=4,
        n_rays=config.train.num_rays,
        val_chunk_size=config.eval.chunk,
        k_nearest_cams=config.dataset.k_nearest_cams
    )

    # 3. åˆ›å»º Trainerï¼Œé…ç½® Logger å’Œ Checkpoint è·¯å¾„
    devices = 2
    trainer = L.Trainer(
        max_steps=config.train.max_steps,
        accelerator="gpu",
        devices=devices,  
        strategy="ddp" if devices > 1 else "auto",
        logger=TensorBoardLogger("logs", name=exp_name, version=mode), 
        callbacks=[
            L.pytorch.callbacks.ModelCheckpoint(
                dirpath=checkpoint_dir,
                filename="lfnr-da-{epoch:02d}",
                monitor="epoch",
                mode="max",
                save_top_k=4,
                every_n_epochs=5,
                save_on_train_epoch_end=True
            ),
            L.pytorch.callbacks.LearningRateMonitor(logging_interval="epoch")
        ],
        log_every_n_steps=50,
        check_val_every_n_epoch=10, 
    )

    # æ–­ç‚¹é‡è®­é€»è¾‘ä½¿ç”¨æ–°è·¯å¾„
    last_ckpt = None
    if os.path.exists(checkpoint_dir):
        ckpts = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith('.ckpt')]
        if ckpts:
            last_ckpt = max(ckpts, key=os.path.getmtime)
            print(f"æ£€æµ‹åˆ°æ–­ç‚¹æ–‡ä»¶: {last_ckpt}")

    # å¼€å§‹è®­ç»ƒ
    trainer.fit(model, dm, ckpt_path=last_ckpt)
    
    # æµ‹è¯•
    print("=== å¼€å§‹æµ‹è¯• ===")
    trainer.test(model, dm, ckpt_path=last_ckpt)
    
    end_time = time.time()
    duration = end_time - start_time
    hours = int(duration // 3600)
    minutes = int((duration % 3600) // 60)
    seconds = int(duration % 60)
    
    print(f"=== å®Œæˆ ===")
    print(f"ğŸš€ è¿è¡Œæ€»æ—¶é•¿: {hours}å°æ—¶ {minutes}åˆ† {seconds}ç§’")
